df_V3,
auto_mpg[,2])
output
output[1]
summary(output)
head(output)
?as.data.frame()
output <- as.data.frame(output)
output
head(output)
library(readr)
auto_mpg <- read_table("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/auto-mpg.data-original",
"", escape_double = FALSE, trim_ws = TRUE)
View(auto_mpg)
library(readr)
auto_mpg <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/auto-mpg.data",
".", quote = "\\\"", escape_double = FALSE,
trim_ws = TRUE)
View(auto_mpg)
fileName
read <- function(fileName, separators) {
data <- readLines(con <- file(fileName))
close(con)
records <- sapply(data, strsplit, split=separators)
dataFrame <- data.frame(t(sapply(records,c)))
rownames(dataFrame) <- 1: nrow(dataFrame)
return(as.data.frame(dataFrame,stringsAsFactors = FALSE))
}
output <- read(fileName,"\t ")
output
head(output)
output[0]
output <- read(fileName,"\t| ")
summary(output)
output$X18.0...8....307.0......130.0......3504.......12.0...70...1...chevrolet.chevelle.malibu.
library(readr)
auto_mpg <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/auto-mpg.data-original",
"\t", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(auto_mpg)
head(auto_mpg)
summary(auto_mpg)
dim(auto_mpg)
colnames(auto_mpg) = c ("MPG","Cylinder","Displacement","HorsePower","Weight","Acc","ModelYr","Origin","CarName")
myData <- auto_mpg
na.omit
na.omit(myData)
dim(myData)
dim(auto_mpg)
myData <- na.omit(myData)
dim(auto_mpg)
dim(myData)
lr.model.1 <- lm(MPG~.,data=myData)
summary(lr.model.1)
plot(lr.model.1$fitted.values,lr.model.1$residuals)
plot(Cylinder,lr.model.1$residuals)
plot(myData$Cylinder,lr.model.1$residuals)
plot(myData$Displacement,lr.model.1$residuals)
plot(myData$HorsePower,lr.model.1$residuals)
plot(myData$Weight,lr.model.1$residuals)
plot(myData$Acc,lr.model.1$residuals)
plot(myData$ModelYr,lr.model.1$residuals)
plot(myData$Origin,lr.model.1$residuals)
plot(myData$CarName,lr.model.1$residuals)
summary(lr.model.1)
attach(spiritwt_data)
lr.model_1 = lm(Wght~Temp+Dilution)
summary(lr.model_1)
plot(lr.model_1$fitted.values,lr.model_1$residuals)
plot(Temp, lr.model_1$residuals)
plot(Dilution,lr.model_1$residuals)
plot(lr.model_1$fitted.values,lr.model_1$residuals)
plot(Temp, lr.model_1$residuals)
plot(Dilution,lr.model_1$residuals)
lr.model_2 = lm(Wght~Temp+poly(Dilution,2))
summary(lr.model_2)
plot(Dilution,lr.model_2$residuals)
dim(myData)
library(readr)
auto_mpg <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/auto-mpg.data-original",
"\t", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(auto_mpg)
head(auto_mpg)
summary(auto_mpg)
dim(auto_mpg)
colnames(auto_mpg) = c ("MPG","Cylinder","Displacement","HorsePower","Weight","Acc","ModelYr","Origin","CarName")
myData <- auto_mpg
na.omit
na.omit(myData)
dim(myData)
dim(auto_mpg)
myData <- na.omit(myData)
dim(auto_mpg)
dim(myData)
lr.model.1 <- lm(MPG~.,data=myData)
summary(lr.model.1)
dim(myData)
train_data <- sample(392,350)
lr.model.2 <- lm(MPG~.,data=myData,subset=train_data)
summary(lr.model.2)
mean(lr.model.2$residuals^2)
summary(myData$MPG)
sqrt(mean(lr.model.2$residuals^2))
summary(spiritwt_data$Wght)
train = sample(315,215)
lr.model.temp = lm(Wght~Temp+Dilution, data=spiritwt_data, subset=train)
# R automatically uses only the rows in the train subset to
# create the model
summary(lr.model.temp)
# Still looks pretty good ...
# Here is how we can calculate the training MSE
mean(lr.model.temp$residuals^2)
mean((MPG - predict(lr.model.2,myData))[-train_data]^2)
mean((myData$MPG - predict(lr.model.2,myData))[-train_data]^2)
attach((myData))
mean((MPG - predict(lr.model.2, myData)) [-train_data]^2)
predict(lr.model.2,myData)
predict(lr.model.2,data.frame(myData))
?predict
predict(lr.model.2)
library(readr)
auto_mpg <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/auto-mpg.data-original",
"\t", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(auto_mpg)
head(auto_mpg)
summary(auto_mpg)
dim(auto_mpg)
colnames(auto_mpg) = c ("MPG","Cylinder","Displacement","HorsePower","Weight","Acc","ModelYr","Origin","CarName")
myData <- auto_mpg
myData <- na.omit(myData)
myData$MPG =as.factor(myData$MPG)
myData$Cylinder = as.factor(myData$Cylinder)
myData$Displacement = as.factor(myData$Displacement)
myData$HorsePower = as.factor(myData$HorsePower)
myData$Weight = as.factor(myData$Weight)
myData$Acc = as.factor(myData$Acc)
myData$ModelYr = as.factor(myData$ModelYr)
myData$Origin = as.factor(myData$Origin)
myData$CarName = as.factor(myData$CarName)
lr.model.1 <- lm(MPG~.,data=myData)
summary(lr.model.1)
dim(myData)
summary(myData)
head(myData)
lr.model.1 <- lm(MPG~. -CarName,data=myData)
myData <- na.omit(myData)
lr.model.1 <- lm(MPG~. -CarName,data=myData)
lr.model.1 <- lm(MPG~.,data=myData)
summary(lr.model.1)
auto_mpg <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/auto-mpg.data-original",
"\t", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(auto_mpg)
head(auto_mpg)
summary(auto_mpg)
dim(auto_mpg)
colnames(auto_mpg) = c ("MPG","Cylinder","Displacement","HorsePower","Weight","Acc","ModelYr","Origin","CarName")
myData <- auto_mpg
myData <- na.omit(myData)
lr.model.1 <- lm(MPG~.,data=myData)
lr.model.1 <- lm(MPG~. -CarName,data=myData)
summary(lr.model.1)
plot(lr.model.1$residuals,lr.model.1$fitted.values)
library(readr)
Car_Worth <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/R_CSV/Lecture-12-Data/Car Worth.txt",
"\t", escape_double = FALSE, trim_ws = TRUE)
View(Car_Worth)
myData_car=Car_Worth
myData_car$Make=as.factor(myData_car$Make)
myData_car$Model=as.factor(myData_car$Model)
myData_car$Trim=as.factor(myData_car$Trim)
myData_car$Type=as.factor(myData_car$Type)
myData_car$Cylinder=as.factor(myData_car$Cylinder)
myData_car$Doors=as.factor(myData_car$Doors)
myData_car$Cruise=as.factor(myData_car$Cruise)
myData_car$Sound=as.factor(myData_car$Sound)
myData_car$Leather=as.factor(myData_car$Leather)
summary(myData_car)
# Categorical predictors with many categories are going to be
# a problem - too many dummy variables. Model probably already
# covered by Make. Drop trim too.
lr.model_cards = lm(Price~. -Model -Trim, data=myData_car)
summary(lr.model_cards)
lr.model_cars2 = update(lr.model_cards, .~.-Doors -Leather -Sound -Cruise)
summary(lr.model_cars2)
# Not bad - let's check the residuals
plot(lr.model_cars2$residuals~lr.model_cars2$fitted.values)
plot(lr.model.1$residuals,lr.model.1$fitted.values)
lr.model.2 <- lm(MPG~.,data=myData,subset=train_data)
summary(lr.model.2)
lr.model.2 <- lm(MPG~. -CarName,data=myData,subset=train_data)
summary(lr.model.2)
mean(lr.model.2$residuals^2)
summary(myData$MPG)
sqrt(mean(lr.model.2$residuals^2))
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
mean( (MPG - predict(lr.model.2, myData - CarName) ) [-train_data]^2)
mean( (MPG - predict(lr.model.2, myData[,-8]) ) [-train_data]^2)
mean( (MPG - predict(lr.model.2, myData[,-8]) ) [-train_data[,-8]]^2)
myData = myData[,-8]
head(myData)
myData[,-1]
myData[,-8]
myData = myData[,-8]
head(myData)
mean( (MPG - predict(lr.model.2, myData[,-8]) ) [-train_data^2)
mean( (MPG - predict(lr.model.2, myData[,-8]) ) [-train_data]^2)
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
train_data=sample(392,350)
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
train_data
myData[train_data[1]]
myData[,train_data[1]]
myData[0]
myData["1",]
myData["2",]
myData[2,]
myData[train_data[1],]
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
lr.model.2
lr.model.2 <- lm(MPG~. ,data=myData,subset=train_data)
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
mean(lr.model.2$residuals^2)
library(boot)
lr.model.boot = glm(MPG~., data=myData)
summary(lr.model.boot)
summary(myData)
cv.error.boot = cv.glm(myData,lr.model.boot)
cv.error.boot$delta
for(i in 1:7){
plot(lr.model.2$residuals~ myData[i])
}
for(i in 1:7){
plot(lr.model.2$residuals~ myData[,i])
}
myData[,1]
plot(lr.model.boot$residuals~myData[,1])
plot(lr.model.boot$residuals~myData$Cylinder)
myData$Cylinder
typeof(myData$Cylinder)
typeof(myData[,1])
train_data = sample(392,266) #68%
lr.model.2 <- lm(MPG~. -CarName, data=myData, subset=train_data)
summary(lr.model.2)
mean(lr.model.2$residuals^2)
summary(myData$MPG)  #0.7% of average MPG, so that seems to be okay
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2) #17.04635
#try doing with LOOCV
library(boot)
lr.model.boot = glm(MPG~., data=myData)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData,lr.model.boot)
cv.error.boot$delta
myData = myData[,-8]
train_data = sample(392,266) #68%
lr.model.2 <- lm(MPG~. -CarName, data=myData, subset=train_data)
summary(lr.model.2)
mean(lr.model.2$residuals^2)
summary(myData$MPG)  #0.7% of average MPG, so that seems to be okay
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2) #17.04635
#try doing with LOOCV
library(boot)
lr.model.boot = glm(MPG~., data=myData)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData,lr.model.boot)
cv.error.boot$delta
head(myData)
train_data = sample(392,266) #68%
lr.model.2 <- lm(MPG~., data=myData, subset=train_data)
summary(lr.model.2)
mean(lr.model.2$residuals^2)
summary(myData$MPG)  #0.7% of average MPG, so that seems to be okay
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
lr.model.boot = glm(MPG~., data=myData)
cv.error.boot = cv.glm(myData,lr.model.boot)
cv.error.boot$delta
mean( (MPG - predict(lr.model.2, myData) ) [-train_data]^2)
cv.error.kcv <- cv.glm(myData,lr.model.boot,K=10)
cv.error.kcv
cv.error.kcv$delta
test_data <- myData[-train_data]
myData[-train_data]
myData[-train_data[1]]
test_data <- myData[-train_data,]
test_data
predict(lr.model.2,test_data)
x_test <- test_data[,2:7]
y_test <-test_data[,1]
predict(lr.model.2,x_test)
mean( test_data$MPG - predict(lr.model.2,test_data)  )
mean( (test_data$MPG - predict(lr.model.2,test_data))^2  )
kappa(myData)
sample <- model.matrix(~myData$MPG+myData$Cylinder)
kappa(sample)
sample <- model.matrix(~myData$MPG+myData$Cylinder+myData$Displacement)
kappa(sample)
myData_car$Make=as.factor(myData_car$Make)
myData_car$Model=as.factor(myData_car$Model)
myData_car$Trim=as.factor(myData_car$Trim)
myData_car$Type=as.factor(myData_car$Type)
myData_car$Cylinder=as.factor(myData_car$Cylinder)
myData_car$Doors=as.factor(myData_car$Doors)
myData_car$Cruise=as.factor(myData_car$Cruise)
myData_car$Sound=as.factor(myData_car$Sound)
myData_car$Leather=as.factor(myData_car$Leather)
summary(myData_car)
lr.model_cards = lm(Price~. -Model -Trim, data=myData_car)
summary(lr.model_cards)
library(readr)
ENB2012_data <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/R_CSV/Lecture-13-Data/ENB2012_data.txt",
"\t", escape_double = FALSE, trim_ws = TRUE)
View(ENB2012_data)
myData_enb = ENB2012_data[,-11]
myData_enb = myData_enb[,-11]
myData_enb = na.omit(myData_enb)
summary(myData_enb)
dim(myData_enb)
myData_enb$X8 = as.factor(myData_enb$X8)
myData_enb$X6 = as.factor(myData_enb$X6)
summary(myData_enb)
glm.fit = glm(Y1~.-Y2, data=myData_enb)
summary(glm.fit)
# We see X6 is not significant, and X4 is collinear
glm.fit = glm(Y1~.-Y2 -X6 -X4, data=myData_enb)
summary(glm.fit)
cv.out=cv.glm(myData_enb,glm.fit, K=10)
cv.out$delta[1]
summary(myData_enb$Y1)
# SQRT(MSE) is about 12.6% of the average value of Y1
plot(glm.fit$residuals~glm.fit$fitted.values)
summary(myData)
summary(lr.model.boot)
lr.model.boot = glm(MPG~. -Cylinder -Displacement -HorsePower -Acc, data=myData)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData,lr.model.boot)
cv.error.boot$delta #12.08526
cv.error.kcv <- cv.glm(myData,lr.model.boot,K=10)
cv.error.kcv$delta #12.20015
summary(myData)
summary(myData$MPG)
plot(lr.model.boot$residuals ~lr.model.boot$fitted.values)
plot(glm.fit$residuals~glm.fit$fitted.values)
library(readr)
student_por <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/student/student-por.csv",
";", escape_double = FALSE, trim_ws = TRUE)
View(student_por)
myData_grade <- student_por
summary(myData_grade)
myData_grade <- as.data.frame(student_por)
lm(myData_grade$G3~.,data=myData_grade)
dim(myData_grade)
train_data = sample(649,441) #68%
lr.model.2 <- lm(G3~., data=myData_grade, subset=train_data)
summary(lr.model.2)
mean(lr.model.2$residuals^2)
summary(myData_grade$G3)
mean( (G3 - predict(lr.model.2, myData_grade) ) [-train_data]^2) #9.9745
mean( (myData_grade$G3 - predict(lr.model.2, myData_grade) ) [-train_data]^2) #9.9745
lr.model.boot = glm(myData_grade$G3~., data=myData_grade)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData_grade,lr.model.boot)
cv.error.boot$delta #12.08526
cv.error.boot = cv.glm(myData,lr.model.boot)
cv.error.boot = cv.glm(myData_grade,lr.model.boot)
myData_grade$school
len(lr.model.boot)
typeof(myData_grade$school)
lr.model.boot = glm(myData_grade$G3~., data=myData_grade)
summary(lr.model.boot)
lr.model.boot
cv.error.boot = cv.glm(myData_grade,lr.model.boot)
lr.model.boot = glm(MPG~., data=myData)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData,lr.model.boot)
lr.model.boot = glm(myData_grade$G3~., data=myData_grade)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData_grade,lr.model.boot)
lr.model.boot = glm(G3~., data=myData_grade)
summary(lr.model.boot)
cv.error.boot = cv.glm(myData_grade,lr.model.boot)
cv.error.boot$delta #11.835
cv.error.kcv <- cv.glm(myData_grade,lr.model.boot,K=10)
cv.error.kcv$delta #12.20015 - 14.8%
library(readr)
library(leaps)
library(ISLR)
OnlineNewsPopularity <- read_csv("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/OnlineNewsPopularity/OnlineNewsPopularity.csv")
myData = OnlineNewsPopularity
#remove url and timedelta vectors
myData <- myData[,-1]
#check to see the vectors that have to be a factor:
myData$data_channel_is_lifestyle = as.factor(myData$data_channel_is_lifestyle)
myData$data_channel_is_bus = as.factor(myData$data_channel_is_bus)
myData$data_channel_is_entertainment = as.factor(myData$data_channel_is_entertainment)
myData$data_channel_is_socmed = as.factor(myData$data_channel_is_socmed)
myData$data_channel_is_tech = as.factor(myData$data_channel_is_tech)
myData$data_channel_is_world = as.factor(myData$data_channel_is_world)
myData$weekday_is_monday =as.factor(myData$weekday_is_monday)
myData$weekday_is_tuesday =as.factor(myData$weekday_is_tuesday)
myData$weekday_is_wednesday =as.factor(myData$weekday_is_wednesday)
myData$weekday_is_thursday =as.factor(myData$weekday_is_thursday)
myData$weekday_is_friday =as.factor(myData$weekday_is_friday)
myData$weekday_is_saturday =as.factor(myData$weekday_is_saturday)
myData$weekday_is_sunday =as.factor(myData$weekday_is_sunday)
myData$is_weekend = as.factor(myData$is_weekend)
#lets try forward model selection
news.model.fwd = regsubsets(shares~. , data=myData, method="forward") #1 subsets of each size up to 20
summary(news.model.fwd)
news.model.fwd = regsubsets(shares~., myData, really.big = T, nvmax= 58, method ="forward")
news.fwd.summary = summary(news.model.fwd)
news.fwd.summary$rsq
news.fwd.summary$adjr2
plot(news.fwd.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
plot(news.fwd.summary$cp,xlab="Number of Variables",ylab="Cp",type="l")
plot(news.model.fwd,scale="adjr2")
news.fwd.summary$cp
coef(news.model.fwd,28)
#do a simple CV since regsubsets does not support CV
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(myData),rep=TRUE)
test = (!train)
regfit.best=regsubsets(shares~.,data=myData[train,],nvmax=19,really.big = T,method="forward")
# Problem - the predict function does not work with resubsets ...
# We need to create our own predict function
# Here is the model matrix - it contains the equation fof the (full) model
test.mat=model.matrix(shares~.,data=myData[test,])
val.errors=rep(NA,19)
for(i in 1:19){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((myData$shares[test]-pred)^2)
}
val.errors
# Now we can check which model gave the lowest MSE
which.min(val.errors)
coef(regfit.best,15)
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
#k - fold
regfit.best=regsubsets(shares~.,data=myData,nvmax=19, really.big = T, method = "forward")
k=10
set.seed(1)
folds=sample(1:k,nrow(myData),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
# This matrix will hold the MSE for each fold, for each model
for(j in 1:k){
best.fit=regsubsets(shares~.,data=myData[folds!=j,],nvmax=19,method = "forward", really.big = T)
for(i in 1:19){
pred=predict(best.fit,myData[folds==j,],id=i)
cv.errors[j,i]=mean( (myData$shares[folds==j]-pred)^2)
}
# Now we get the average over all the folds for each model size
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
plot(mean.cv.errors,type='b') # looks like 15 variables is the best
reg.best=regsubsets(shares~.,data=myData, nvmax=19, really.big = T,method = "forward")
coef(reg.best,15)
library(glmnet)
summary(Hitters)
x = model.matrix(shares~., myData)[,-1]
x
dim(x)
x=model.matrix(shares~.,myData)[,-1]
y=myData$shares
# We create a sequence of lambdas to test
grid=10^seq(10,-2,length=100)
# We now do Ridge reqression for each lambda
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
names(ridge.mod)
ridge.mod$lambda [50]
coef(ridge.mod)[,50]
predict(ridge.mod,s=50,type="coefficients")[1:20,]
# We would like to choose an optimal lambda - we'll demonstrate
# a simple CV method here (K-fold can be used with more work).
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
# We need to find the optimal lambda - we can use CV
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
# The cv function does 10-fold CV by default
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.model=glmnet(x,y,alpha=0)
predict(ridge.model,type="coefficients",s=bestlam)[1:20,]
# Note all coef are included, but some a weighted heavier than others
# Now we apply the Lasso - note all we do is change the alpha option
lasso.mod =glmnet (x[train ,],y[train],alpha =1, lambda =grid)
plot(lasso.mod)
set.seed (1)
cv.out =cv.glmnet (x[train ,],y[train],alpha =1)
plot(cv.out)
bestlam =cv.out$lambda.min
lasso.pred=predict (lasso.mod,s=bestlam,newx=x[test,])
mean(( lasso.pred -y.test)^2)
out=glmnet (x,y,alpha =1, lambda =grid)
lasso.coef=predict(out,type ="coefficients",s=bestlam )[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
