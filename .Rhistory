f
nn.fit = neuralnet(f,data=trainingData,hidden=c(20),linear.output=TRUE)
summary(naOmit.myData)
library(readr)
library(ISLR)
library(pls)
#read the dataset
AmesHousing <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/Ames Housing/AmesHousing.txt", "\t", escape_double = FALSE, trim_ws = TRUE)
#copy it to myData and remove order and PID initially
myData = AmesHousing
myData$Order = NULL
myData$PID = NULL
#pre -process data - remove the predictors, NA and convert to factors
# Can be Factored? -  Year.Build?
myData = as.data.frame(unclass(myData))
#myData[is.na(myData)]<-0
myData$Alley = NULL
myData$Street = NULL
myData$Utilities = NULL
myData$Condition.2 = NULL
myData$Roof.Matl = NULL
myData$BsmtFin.SF.2 = NULL
myData$Heating = NULL
myData$Bsmt.Half.Bath = NULL
myData$Bsmt.Full.Bath = NULL
myData$Low.Qual.Fin.SF = NULL
myData$Fireplaces = NULL
myData$Pool.Area = NULL
myData$Screen.Porch = NULL
myData$X3Ssn.Porch = NULL
myData$Wood.Deck.SF = NULL
myData$Open.Porch.SF=NULL
myData$Enclosed.Porch = NULL
myData$Pool.QC = NULL
myData$Fence = NULL
myData$Misc.Feature = NULL
myData$Misc.Val = NULL
#extra needed
myData$MS.Zoning = NULL
myData$Land.Contour = NULL
myData$Land.Slope = NULL
myData$Condition.1 = NULL
myData$Bldg.Type = NULL
myData$Roof.Style = NULL
myData$Mas.Vnr.Area = NULL
myData$Exter.Cond = NULL
myData$Bsmt.Cond = NULL
myData$BsmtFin.Type.2 = NULL
myData$Half.Bath = NULL
myData$X2nd.Flr.SF = NULL
myData$Electrical = NULL
myData$Central.Air = NULL
myData$Fireplace.Qu = NULL
myData$Garage.Qual = NULL
myData$Garage.Cond = NULL
myData$Paved.Drive = NULL
myData$Overall.Cond = NULL
myData$Full.Bath =NULL
myData$Kitchen.AbvGr = NULL
myData$Lot.Shape =  NULL#as.numeric(myData$Lot.Shape)
myData$Lot.Config = NULL#as.numeric(myData$Lot.Config)
myData$Neighborhood = NULL#as.numeric(myData$Neighborhood)
myData$House.Style = NULL#as.numeric(myData$House.Style)
myData$Exterior.1st = NULL#as.numeric(myData$Exterior.1st)
myData$Exterior.2nd = NULL#as.numeric(myData$Exterior.2nd)
myData$Mas.Vnr.Type = NULL#as.numeric(myData$Mas.Vnr.Type)
myData$Exter.Qual = NULL#as.numeric(myData$Exter.Qual)
myData$Foundation = NULL#as.numeric(myData$Foundation)
myData$Bsmt.Qual = NULL#as.numeric(myData$Bsmt.Qual)
myData$Bsmt.Exposure = NULL#as.numeric(myData$Bsmt.Exposure)
myData$BsmtFin.Type.1 = NULL#as.numeric(myData$BsmtFin.Type.1)
myData$Heating.QC = NULL#as.numeric(myData$Heating.QC)
myData$Kitchen.Qual = NULL#as.numeric(myData$Kitchen.Qual)
myData$Functional = NULL#as.numeric(myData$Functional)
myData$Garage.Type = NULL#as.numeric(myData$Garage.Type)
myData$Garage.Finish = NULL#as.numeric(myData$Garage.Finish)
myData$Sale.Condition = NULL#as.numeric(myData$Sale.Condition)
myData$Sale.Type = NULL#as.numeric(myData$Sale.Type)
myData$MS.SubClass = NULL#as.numeric(myData$MS.SubClass)
dim(myData)
naOmit.myData = na.omit(myData)
naOmit.myData<-naOmit.myData[!(naOmit.myData$Total.Bsmt.SF>=4000),] # remove sqft above 4000 - outliers
naOmit.myData<-naOmit.myData[!(naOmit.myData$Gr.Liv.Area>=4000),] # remove Gr.Liv.Area outliers - max is 5642, 3rd QU : 1734
dim(naOmit.myData)   #losing about 25% of the dataset - comes from 2930 to 2220
naOmit.myData <- na.omit(naOmit.myData)
maxs = apply(naOmit.myData, 2, max)
mins = apply(naOmit.myData, 2, min)
# These are vectors of the max and min values for each column
myDataScaled = as.data.frame(naOmit.myData)
index = sample(1:nrow(myDataScaled),round(0.75*nrow(myDataScaled)))
trainingData = myDataScaled[index,]
testData = myDataScaled[-index,]
n = names(trainingData)
n
f = as.formula(paste("SalePrice ~", paste(n[!n %in% "SalePrice"], collapse = " + ")))
f
nn.fit = neuralnet(f,data=trainingData,hidden=c(20),linear.output=TRUE)
dim(naOmit.myData)
n = names(trainingData)
n
f = as.formula(paste("SalePrice ~", paste(n[!n %in% "SalePrice"], collapse = " + ")))
f
nn.fit = neuralnet(f,data=trainingData,hidden=c(10),linear.output=TRUE)
myDataClean = naOmit.myData
plot(nn.fit)
# Cool - how did it do?
nn.pred = compute(nn.fit,testData[,1:17])
names(nn.pred)
nn.MSE = sum((testData$SalePrice - nn.pred$net.result)^2)/nrow(testData)
nn.MSE
# Comparable. Let's recale and see how accurate this is
nn.predUnscale = nn.pred$net.result*(max(myDataClean$SalePrice)-min(myDataClean$SalePrice))+min(myDataClean$SalePrice)
testMedvUnscale = (testData$SalePrice)*(max(myDataClean$SalePrice)-min(myDataClean$SalePrice))+min(myDataClean$SalePrice)
MSEUnscaled = sum((testMedvUnscale - nn.predUnscale)^2)/nrow(testData)
sqrt(MSEUnscaled)
summary(myDataClean$SalePrice)
sqrt(MSEUnscaled)/mean(myDataClean$SalePrice)
mlp
?mx.mlp
library(mlbench)
install.packages(mlbench,dependencies = TRUE)
install.packages("mlbench",dependencies = TRUE)
install.packages("mxnet",dependencies = TRUE)
install.packages("methods",dependencies = TRUE)
install.packages("methods", dependencies = TRUE)
?mx.mlp
??ma.mp
?mx.mlp
??mx.mlp
library(mlbench)
library(mxnet)
library("mxnet")
??mx.mlp
library(methods)
??mx.mlp
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
install.packages("mxnet",dependencies = TRUE)
dim(naOmit.myData)
nn.fit
nn.pred
names(nn.pred)
nn.MSE
plot(nn.fit)
nn.fit = neuralnet(f,data=trainingData,hidden=c(10),linear.output=TRUE)
plot(nn.fit)
nn.fit = neuralnet(f,data=trainingData,hidden=c(3),linear.output=TRUE)
install.packages(tm,dependencies = TRUE)
install.packages("tm",dependencies = TRUE)
library("tm")
tm_map()
?tm_map
stopwords("english")
install.packages("caTools", dependencies = TRUE)
library("randomForest")
library("caTools")
library("recommenderlab")
install.packages("recommenderlab",dependencies = TRUE)
library("reshape2")
?acast
library(recommenderlab)
library(reshape2)
?acast
library(readr)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
"\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
library(tm)
corpus <- Corpus(VectorSource(SMSSpamCollection$X2))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
important_words_df <- as.data.frame(as.matrix(sparse))
library(caTools)
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(SMSSpamCollection,important_words_df)
myData$X2 = NULL
spl <- sample.split(myData$X1, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
dim(myData)
lr.model = glm(X1~., data=myData_train, family=binomial)
library(readr)
library(tm)
library(caTools)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
+     "\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
SMSSpamCollection$X1[SMSSpamCollection$X1=="spam"] <- 1
SMSSpamCollection$X1[SMSSpamCollection$X1=="ham"] <- 0
# First we create the corpus
corpus <- Corpus(VectorSource(SMSSpamCollection$X2))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
# Now we create the DF ...
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(SMSSpamCollection,important_words_df)
myData$X2 = NULL
spl <- sample.split(myData$X1, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
lr.model = glm(X1~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$X1,lr.probs>.5)
mean(myData_test$X1 == (lr.probs>.5))
lr.err = mean(myData_test$X1 == (lr.probs>.5))
head(myData_train)
head(myData)
lr.model = glm(X1~., data=myData_train, family=binomial)
library("randomForest")
rf.model = randomForest(X1~., data=myData_train,mtry=6,importance=TRUE)
head(myData_train)
typeof(myData_train$X1)
?transform
transform(myData_train, X1 = as.numeric(myData_train$X1))
typeof(myData_train$X1)
transform(myData_train, X1 = as.numeric(X1))
typeof(myData_train$X1)
myData_train <- transform(myData_train, X1 = as.numeric(X1))
typeof(myData_train$X1)
?as.numeric
myData_train <- transform(myData_train, X1 = as.integer(X1))
typeof(myData_train$X1)
library(readr)
library(tm)
library(caTools)
library(randomForest)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
+     "\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
SMSSpamCollection$X1[SMSSpamCollection$X1=="spam"] <- 1
SMSSpamCollection$X1[SMSSpamCollection$X1=="ham"] <- 0
SMSSpamCollection <- transform(SMSSpamCollection, X1 = as.integer(X1))
# First we create the corpus
corpus <- Corpus(VectorSource(SMSSpamCollection$X2))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
# Now we create the DF ...
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(SMSSpamCollection,important_words_df)
myData$X2 = NULL
spl <- sample.split(myData$X1, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
lr.model = glm(X1~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$X1,lr.probs>.5)
mean(myData_test$X1 == (lr.probs>.5))
lr.err = mean(myData_test$X1 == (lr.probs>.5))
rf.model = randomForest(X1~., data=myData_train,mtry=6,importance=TRUE)
myData_train_rf=myData_train
myData_train_rf$sentiment=as.factor(myData_train_rf$sentiment)
rf.model = randomForest(sentiment~., data=myData_train_rf,mtry=6,importance=TRUE)
rf.pred = predict(rf.model,myData_test, type="class")
table(rf.pred,myData_test$sentiment)
mean(myData_test$sentiment == rf.pred)
lr.err
library(readr)
library(tm)
library(caTools)
library(randomForest)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
+     "\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
SMSSpamCollection$X1[SMSSpamCollection$X1=="spam"] <- 1
SMSSpamCollection$X1[SMSSpamCollection$X1=="ham"] <- 0
SMSSpamCollection <- transform(SMSSpamCollection, X1 = as.numeric(X1))
# First we create the corpus
corpus <- Corpus(VectorSource(SMSSpamCollection$X2))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
# Now we create the DF ...
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(SMSSpamCollection,important_words_df)
myData$X2 = NULL
spl <- sample.split(myData$X1, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
lr.model = glm(X1~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$X1,lr.probs>.5)
mean(myData_test$X1 == (lr.probs>.5))
lr.err = mean(myData_test$X1 == (lr.probs>.5))
rf.model = randomForest(X1~., data=myData_train,mtry=6,importance=TRUE)
myData_train_rf=myData_train
myData_train_rf$sentiment=as.factor(myData_train_rf$sentiment)
rf.model = randomForest(sentiment~., data=myData_train_rf,mtry=6,importance=TRUE)
rf.pred = predict(rf.model,myData_test, type="class")
table(rf.pred,myData_test$sentiment)
mean(myData_test$sentiment == rf.pred)
head(myData_train)
library(readr)
library(tm)
library(caTools)
library(randomForest)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
+     "\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
SMSSpamCollection$X1[SMSSpamCollection$X1=="spam"] <- 1
SMSSpamCollection$X1[SMSSpamCollection$X1=="ham"] <- 0
typeof(SMSSpamCollection$X1)
library(readr)
library(tm)
library(caTools)
library(randomForest)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
+     "\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
SMSSpamCollection$X1[SMSSpamCollection$X1=="spam"] <- "1"
SMSSpamCollection$X1[SMSSpamCollection$X1=="ham"] <- "0"
typeof(SMSSpamCollection$X1)
SMSSpamCollection <- transform(SMSSpamCollection, X1 = as.numeric(X1))
typeof(SMSSpamCollection$X1)
View(SMSSpamCollection)
corpus <- Corpus(VectorSource(SMSSpamCollection$X2))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
# Now we create the DF ...
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(SMSSpamCollection,important_words_df)
myData$X2 = NULL
spl <- sample.split(myData$X1, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
lr.model = glm(X1~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$X1,lr.probs>.5)
mean(myData_test$X1 == (lr.probs>.5))
lr.err = mean(myData_test$X1 == (lr.probs>.5))
rf.model = randomForest(X1~., data=myData_train,mtry=6,importance=TRUE)
myData_train_rf=myData_train
myData_train_rf$sentiment=as.factor(myData_train_rf$sentiment)
myData_train_rf$sentiment=as.factor(myData_train_rf$X1)
rf.model = randomForest(X1~., data=myData_train_rf,mtry=6,importance=TRUE)
rf.pred = predict(rf.model,myData_test, type="class")
rf.model = randomForest(X1~., data=myData_train,mtry=6,importance=TRUE)
myData_train_rf=myData_train
myData_train_rf$X1=as.factor(myData_train_rf$X1)
rf.model = randomForest(X1~., data=myData_train_rf,mtry=6,importance=TRUE)
rf.pred = predict(rf.model,myData_test, type="class")
table(rf.pred,myData_test$sentiment)
table(rf.pred,myData_test$X1)
mean(myData_test$sentiment == rf.pred)
mean(myData_test$X1 == rf.pred)
library(readr)
library(tm)
library(caTools)
library(randomForest)
SMSSpamCollection <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/SMSspamcollection/SMSSpamCollection",
+     "\t", escape_double = FALSE, col_names = FALSE)
View(SMSSpamCollection)
SMSSpamCollection$X1[SMSSpamCollection$X1=="spam"] <- "0"
SMSSpamCollection$X1[SMSSpamCollection$X1=="ham"] <- "1"
SMSSpamCollection <- transform(SMSSpamCollection, X1 = as.numeric(X1))
# First we create the corpus
corpus <- Corpus(VectorSource(SMSSpamCollection$X2))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
# Now we create the DF ...
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(SMSSpamCollection,important_words_df)
myData$X2 = NULL
spl <- sample.split(myData$X1, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
lr.model = glm(X1~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$X1,lr.probs>.5)
mean(myData_test$X1 == (lr.probs>.5))
lr.err = mean(myData_test$X1 == (lr.probs>.5))
rf.model = randomForest(X1~., data=myData_train,mtry=6,importance=TRUE)
myData_train_rf=myData_train
myData_train_rf$X1=as.factor(myData_train_rf$X1)
rf.model = randomForest(X1~., data=myData_train_rf,mtry=6,importance=TRUE)
rf.pred = predict(rf.model,myData_test, type="class")
table(rf.pred,myData_test$X1)
mean(myData_test$X1 == rf.pred)
library(readr)
movie_review <- read_csv("~/Downloads/Education/Spring 17/R for Data Scientists/R_CSV/Lecture-24-Data/movie_review.txt")
View(movie_review)
corpus <- Corpus(VectorSource(movie_review$review))
# Now we do the pre-processing of the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)
dtm
sparse <- removeSparseTerms(dtm, 0.90)
sparse
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(movie_review,important_words_df)
myData$id=NULL
myData$review=NULL
spl <- sample.split(myData$sentiment, .85)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)
# Let's try LR again:
lr.model = glm(sentiment~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$sentiment,lr.probs>.5)
mean(myData_test$sentiment == (lr.probs>.5))
lr.err = mean(myData_test$sentiment == (lr.probs>.5))
library("randomForest", lib.loc="~/R/win-library/3.3")
rf.model = randomForest(sentiment~., data=myData_train,mtry=6,importance=TRUE)
myData_train_rf=myData_train
myData_train_rf$sentiment=as.factor(myData_train_rf$sentiment)
rf.model = randomForest(sentiment~., data=myData_train_rf,mtry=6,importance=TRUE)
rf.pred = predict(rf.model,myData_test, type="class")
table(rf.pred,myData_test$sentiment)
mean(myData_test$sentiment == rf.pred)
lr.err
library(readr)
TrainingData <- read_delim("~/Downloads/Education/Spring 17/R for Data Scientists/Assignments/FinalData/TrainingData.txt",
"\t", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(TrainingData)
dim(TrainingData)
train_pca <-prcomp(TrainingData)
biplot(train_pca,cex=c(1/3,1/2), scale=0)
plot(train_pca,type = "l")
# Predict PCs
predict(train_pca,
newdata=tail(log.ir, 2))
dim(TrainingData)
log.train <- log(TrainingData[, 1:85])
predict.train <- TrainingData[,86]
train_pca <-prcomp(TrainingData)
plot(train_pca,type = "l")
predict(train_pca, TrainingData = tail (log.train,2))
TrainingData=tail(log.ir, 2))
dim(TrainingData)
log.train <- log(TrainingData[, 1:85])
predict.train <- TrainingData[,86]
train_pca <-prcomp(TrainingData)
plot(train_pca,type = "l")
predict(train_pca, TrainingData = tail (log.train,2))
